/*
 * Copyright (C) 2020 DBC A/S (http://dbc.dk/)
 *
 * This is part of pg-queue-replayer
 *
 * pg-queue-replayer is free software: you can redistribute it and/or modify
 * it under the terms of the GNU Affero General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * pg-queue-replayer is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU Affero General Public License
 * along with this program.  If not, see <http://www.gnu.org/licenses/>.
 */
package dk.dbc.pgqueue.replayer;

import dk.dbc.commons.testcontainers.postgres.DBCPostgreSQLContainer;
import dk.dbc.pgqueue.common.DatabaseMigrator;
import dk.dbc.pgqueue.supplier.PreparedQueueSupplier;
import dk.dbc.pgqueue.supplier.QueueSupplier;
import dk.dbc.pgqueue.consumer.JobMetaData;
import dk.dbc.pgqueue.consumer.QueueWorker;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSetMetaData;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashSet;
import java.util.List;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.Timeout;

import static dk.dbc.pgqueue.replayer.GenericJobMapper.QUEUE_ADMIN_COLUMNS;
import static org.hamcrest.CoreMatchers.*;
import static org.hamcrest.MatcherAssert.*;

/**
 *
 * @author Morten BÃ¸geskov (mb@dbc.dk)
 */
public class GenericJobMapperIT {

    private static final DBCPostgreSQLContainer PG = makePG();

    private static DBCPostgreSQLContainer makePG() {
        DBCPostgreSQLContainer pg = new DBCPostgreSQLContainer();
        pg.start();
        return pg;
    }

    @Test
    @Timeout(value = 10, unit = TimeUnit.SECONDS)
    public void verifyAdminColumns() throws Exception {
        System.out.println("verifyAdminColumns");

        try (Connection connection = PG.createConnection();
             Statement stmt = connection.createStatement()) {
            stmt.executeUpdate("DROP SCHEMA PUBLIC CASCADE");
            stmt.executeUpdate("CREATE SCHEMA PUBLIC");
            stmt.executeUpdate("CREATE TABLE queue()");
            stmt.executeUpdate("CREATE TABLE queue_error()");
        }
        DatabaseMigrator.migrate(PG.datasource());

        HashSet<String> actual = new HashSet<>();
        try (Connection connection = PG.createConnection();
             PreparedStatement stmt = connection.prepareStatement("SELECT * FROM queue")) {
            ResultSetMetaData metaData = stmt.getMetaData();
            int columnCount = metaData.getColumnCount();
            for (int i = 1 ; i <= columnCount ; i++) {
                String column = metaData.getColumnName(i);
                actual.add(column);
            }
        }
        try (Connection connection = PG.createConnection();
             Statement stmt = connection.createStatement()) {
            stmt.executeUpdate("DROP SCHEMA PUBLIC CASCADE");
            stmt.executeUpdate("CREATE SCHEMA PUBLIC");
        }

        assertThat("Has the columns generated by migrate changed?",
                   actual, is(QUEUE_ADMIN_COLUMNS));
    }

    @Test
    @Timeout(value = 10, unit = TimeUnit.SECONDS)
    public void testMappingAndQueueDequeue() throws Exception {
        System.out.println("testMappingAndQueueDequeue");
        String queueName = "my-queue";

        try (Connection connection = PG.createConnection();
             Statement stmt = connection.createStatement()) {
            stmt.executeUpdate("DROP SCHEMA PUBLIC CASCADE");
            stmt.executeUpdate("CREATE SCHEMA PUBLIC");
            stmt.executeUpdate("CREATE TABLE queue( ape TEXT, badger INT, catapillar JSONB )");
            stmt.executeUpdate("CREATE TABLE queue_error AS SELECT * FROM queue");
        }
        DatabaseMigrator.migrate(PG.datasource());

        GenericJobMapper mapper;
        try (Connection connection = PG.createConnection()) {
            mapper = GenericJobMapper.from(connection);
        }

        assertThat(mapper.columnList(), is(new String[] {"ape", "badger", "catapillar"}));

        List<String[]> jobs = Arrays.asList(new String[] {"one", "1", "{}"},
                                            new String[] {"two", "2", "{\"a\": true}"}); // Space is needed after colon - this is what JSONB looks like when fetched as string

        try (Connection connection = PG.createConnection()) {
            connection.setAutoCommit(true);
            PreparedQueueSupplier<String[]> supplier = new QueueSupplier<>(mapper)
                    .preparedSupplier(connection);
            for (String[] job : jobs) {
                supplier.enqueue(queueName, job);
            }
        }

        List<String[]> dequeued = new ArrayList<>(2);
        QueueWorker worker = QueueWorker.builder(mapper)
                .consume(queueName)
                .dataSource(PG.datasource())
                .emptyQueueSleep(10L)
                .maxTries(1)
                .window(100)
                .build((Connection connection, String[] job, JobMetaData metaData) -> {
                    synchronized (dequeued) {
                        dequeued.add(job);
                        dequeued.notifyAll();
                    }
                });
        worker.start();
        synchronized (dequeued) {
            while (dequeued.size() != 2) {
                dequeued.wait(1000L);
            }
        }
        worker.stop();

        assertThat(dequeued.stream().map(Arrays::toString).collect(Collectors.toList()),
                   is(jobs.stream().map(Arrays::toString).collect(Collectors.toList())));
    }

}
